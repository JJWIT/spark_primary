(,1)
(,1)
(,1)
(        	可以设置Application name。,1)
(        	可以设置运行模式及资源需求。,1)
(       Spark支持文本文件，SequenceFiles和任何其他Hadoop InputFormat。,1)
(    -- Spark可以从Hadoop支持的任何存储源创建分布式数据集，包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3等.,1)
(    -- 修改scala sdk 与 spark 版本兼容,1)
(    -- 原因，spark源码中找winutils这个文件。windows中该文件用于模拟shell环境的,1)
(    -- 并行集合通过调用创建SparkContext的parallelize一个现有的收集方法,1)
(    -- 解决方案：https://stackoverflow.com/questions/35652665/java-io-ioexception-could-not-locate-executable-null-bin-winutils-exe-in-the-ha/46038984,1)
(    1.	创建SparkConf对象,1)
(    2.	创建SparkContext对象,1)
(    3.	基于Spark的上下文创建一个RDD，对RDD进行处理。,1)
(    4.	应用程序中要有Action类算子来触发Transformation类算子执行。,1)
(    5.	关闭Spark上下文对象SparkContext。,1)
(    new SparkContext(conf),1)
(    spark代码流程,1)
(    val conf = new SparkConf().setAppName(appName).setMaster(master),1)
(    val data = Array(1, 2, 3, 4, 5) // 自己创建数据集,1)
(    val distData = sc.parallelize(data),1)
(    val distFile = sc.textFile("data.txt")  //  加载外部数据集,1)
(    ①将winutils.exe放到hadoop_home\bin下,1)
(    ②代码变成添加System.setProperty("hadoop.home.dir", "D:\\WorkSoftware\\hadoop-2.8.5"); 添加hadoop.home.dir位置,1)
(    正常来讲就可以正常运行了，有的时候需要重启电脑，如果还不行需要使用代码如下：,1)
(    记录学习scala、spark相关笔记,1)
(  first=take(1),返回数据集中的第一个元素。,1)
(  作用在K,V格式的RDD上，对key进行升序或者降序排序。,1)
(  先map后flat。与map类似，每个输入项可以映射为0到多个输出项。,1)
(  将一个RDD中的每个数据项，通过map中的函数映射变为一个新的元素。,1)
(  将相同的Key根据相应的逻辑进行处理。,1)
(  将计算结果回收到Driver端。,1)
(  循环遍历数据集中的每个元素，运行相应的逻辑。,1)
(  特点：输入一条，输出一条数据。,1)
(  过滤符合条件的记录数，true保留，false过滤掉。,1)
(  返回一个包含数据集前n个元素的集合。,1)
(  返回数据集中的元素数。会在结果计算完成后回收到Driver端。,1)
(  随机抽样算子，根据传进去的小数按比例进行又放回或者无放回的抽样。,1)
(  	collect,1)
(  	count,1)
(  	filter,1)
(  	first,1)
(  	flatMap,1)
(  	foreach,1)
(  	map,1)
(  	reduceByKey,1)
(  	sample,1)
(  	sortByKey,1)
(  	take(n),1)
(### spark编程(scala),1)
(### 新建spark项目,1)
(### 说明,1)
(### 项目报错,1)
(1.Exception in thread "main" java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps;,1)
(1.使用maven或sbt方式，建议使用maven,1)
(1.初始化spark,1)
(2.参考网址 https://blog.csdn.net/u013963380/article/details/72677212,1)
(2.弹性分布式数据集,1)
(2.运行val sc = new SparkContext(conf)时候报错：java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.,1)
(3.,1)
(3.外部数据集,1)
(4.算子,1)
(====,1)
(Action算子,1)
(Transformations转换算子,1)
(cache、persist、checkpoint,1)
(spark项目 2019-01-30,1)
(控制算子,1)
